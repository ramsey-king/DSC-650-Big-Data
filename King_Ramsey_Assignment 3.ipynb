{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 650 - Big Data\n",
    "# Ramsey King\n",
    "# January 16, 2022\n",
    "# Assignment 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "# from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl_data():\n",
    "    '''s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "    '''\n",
    "    src_data_path = '/Users/ramse/Documents/GitHub/dsc650/data/processed/openflights/routes.jsonl.gz'\n",
    "    with gzip.open(src_data_path, 'rb') as f:\n",
    "        records = [json.loads(line) for line in f.readlines()]\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "        \n",
    "    with open(schema_path, 'r') as f:\n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                ## TODO: Validate record \n",
    "                jsonschema.validate(instance=record, schema=schema)\n",
    "            except ValidationError as e:\n",
    "                ## Print message if invalid record\n",
    "                print(e, 'Given JSON data is INVALID.')\n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    with open('/Users/ramse/Documents/GitHub/dsc650/dsc650/assignments/assignment03/schemas/routes.avsc') as file:\n",
    "        schema = file.read()\n",
    "    schema = json.loads(schema)\n",
    "    with open(data_path, 'wb') as out:\n",
    "        fastavro.writer(out, schema, records)\n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = '/Users/ramse/Documents/GitHub/dsc650/data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    '''s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            pass\n",
    "            ## TODO: Use Apache Arrow to create Parquet table and save the dataset\n",
    "    '''\n",
    "\n",
    "    jsonl_path = '/Users/ramse/Documents/GitHub/dsc650/data/processed/openflights/routes.jsonl'\n",
    "    with open(src_data_path, 'rb') as file:\n",
    "        with gzip.open(file, 'rb') as writer:\n",
    "            df = pd.DataFrame(writer)\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            pq.write_table(table,parquet_output_path)\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-3f43978928b0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     92\u001B[0m         \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msnappy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompress\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroutes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSerializeToString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 94\u001B[1;33m \u001B[0mcreate_protobuf_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-6-3f43978928b0>\u001B[0m in \u001B[0;36mcreate_protobuf_dataset\u001B[1;34m(records)\u001B[0m\n\u001B[0;32m     72\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdst_airport\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m             \u001B[0mroute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mairline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mCopyFrom\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdst_airport\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m         \u001B[0mcodeshare\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_airline_to_proto_obj\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'codeshare'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[0mroute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mairline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcodeshare\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcodeshare\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m         \u001B[0mstops\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_airline_to_proto_obj\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'stops'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-6-3f43978928b0>\u001B[0m in \u001B[0;36m_airline_to_proto_obj\u001B[1;34m(airline)\u001B[0m\n\u001B[0;32m     41\u001B[0m     \u001B[0mobj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mroutes_pb2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAirline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;31m## TODO: Create an Airline obj using Protocol Buffers API\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mairline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mairline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'airline_id'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'bool' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    if not airline.get('name'):\n",
    "        return None\n",
    "    if not airline.get('airline_id'):\n",
    "        return None\n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    obj.name = airline.get('name')\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('callsign'):\n",
    "        obj.callsign = airline.get('callsign')\n",
    "    if airline.get('country'):\n",
    "        obj.country = airline.get('country')\n",
    "    if airline.get('active'):\n",
    "        obj.active = airline.get('active')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        ## TODO: Implement the code to create the Protocol Buffers Dataset\n",
    "        src_airport = _airline_to_proto_obj(record.get('src_airport', {}))\n",
    "        if src_airport:\n",
    "            route.airline.CopyFrom(src_airport)\n",
    "        dst_airport = _airline_to_proto_obj(record.get('dst_airport', {}))\n",
    "        if dst_airport:\n",
    "            route.airline.CopyFrom(dst_airport)\n",
    "        codeshare = _airline_to_proto_obj(record['codeshare'])\n",
    "        route.airline.codeshare = codeshare\n",
    "        stops = _airline_to_proto_obj(record.get('stops', {}))\n",
    "        if stops:\n",
    "            route.airline.CopyFrom(stops)\n",
    "        equipment = _airline_to_proto_obj(record['equipment'])\n",
    "        route.airline.equipment = equipment\n",
    "\n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    for record in records:\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        if src_airport:\n",
    "            latitude = src_airport.get('latitude')\n",
    "            longitude = src_airport.get('longitude')\n",
    "            if latitude and longitude:\n",
    "                ## TODO: use pygeohash.encode() to assign geohashes to the records and complete the hashes list\n",
    "                encoded_lat_long = pygeohash.encode(latitude,longitude)\n",
    "                hashes.append(encoded_lat_long)\n",
    "                record['geohash'] = encoded_lat_long\n",
    "    hashes.sort()\n",
    "    # print(len(set(hashes)))\n",
    "    three_letter = sorted(list(set([entry[:3] for entry in hashes])))\n",
    "    hash_index = {value: [] for value in three_letter}\n",
    "    for record in records:\n",
    "        geohash = record.get('geohash')\n",
    "        if geohash:\n",
    "            hash_index[geohash[:3]].append(record)\n",
    "    for key, values in hash_index.items():\n",
    "        output_dir = geoindex_dir.joinpath(str(key[:1])).joinpath(str(key[:2]))\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        output_path = output_dir.joinpath('{}.jsonl.gz'.format(key))\n",
    "        with gzip.open(output_path, 'w') as f:\n",
    "            json_output = '\\n'.join([json.dumps(value) for value in values])\n",
    "            f.write(json_output.encode('utf-8'))\n",
    "    return list(set(hashes))\n",
    "\n",
    "hash_list = create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest airport is Eppley Airfield at a distance of 19.545 kilometers.\n",
      "The closest airport is Charlotte Douglas International Airport at a distance of 3.803 kilometers.\n"
     ]
    }
   ],
   "source": [
    "def airport_search(latitude, longitude, hashes_to_check, distance_in_km):\n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "    distance_in_m = distance_in_km*1000\n",
    "    inputted_geohash = pygeohash.encode(latitude, longitude)\n",
    "    geo_hash_distance_list = []\n",
    "    counter = 0\n",
    "    for hash_ in hashes_to_check:\n",
    "        geo_hash_distance_list.append(pygeohash.geohash_approximate_distance(inputted_geohash,hash_))\n",
    "        if pygeohash.geohash_approximate_distance(inputted_geohash,hash_) == min(geo_hash_distance_list):\n",
    "            closest_hash = hash_\n",
    "            closest_distance = pygeohash.geohash_approximate_distance(inputted_geohash,hash_)\n",
    "    if min(geo_hash_distance_list) <= distance_in_m:\n",
    "        for record in records:\n",
    "            if closest_hash == record.get('geohash'):\n",
    "                print('The closest airport is', record.get('src_airport').get('name'), 'at a distance of', closest_distance/1000, 'kilometers.')\n",
    "                counter += 1\n",
    "            if counter >=1:\n",
    "                break\n",
    "    else:\n",
    "        print(\"No airport found in the distance specified.\")\n",
    "\n",
    "airport_search(41.1499988, -95.91779, hash_list, 19.546)\n",
    "airport_search(35.23800, -80.91865, hash_list, 200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}